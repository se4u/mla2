Subject of piazza post Submission 2 from xxx
make hw2-e823ab.zip
Write writeup.pdf
all printing must be done as %.13e
estimate parameters of cpd-grid15x15-t100.txt and cpd-grid30x30-t100.txt
syntax is estimate-params network-grid training-grid cpd-grid
test estimate-params by estimate-params test_est_params_network.txt test_est_params_training.txt test_est_params_cpd.txt && diff test_est_params_cpd.txt test_est_params_cpd.txt.gold is empty
alias t=estimate-params test_est_params_network.txt test_est_params_training.txt test_est_params_cpd.txt
I printed out the parameters and then piped to tmp then did a diff to check that I am not missing out any parameters

The total parameters are 31260
diff <(sort test_est_params_cpd.txt) <(sort tmp) | grep "^>" | wc


Question 4.1.4
The code works mainly by counting all the observed variables in their contexts. It is completely contained in estimate_params.py and the estimate-params script is a wrapper on top of the script.
The following call in the source directory produces the required outputs.
./estimate-params f/network-grid15x15-t100.txt f/training-grid15x15-t100.txt cpd-grid15x15-t100.txt
./estimate-params f/network-grid30x30-t100.txt f/training-grid30x30-t100.txt cpd-grid30x30-t100.txt
The main calls are
transition_probs=get_transition_probs_from_the_insane_input_file(training_file_name)
observation_prob=get_observation_prob_from_insane_input_file(training_file_name)

Question 4.1.5
Part 1.
If we dont do parameter sharing then we would have to get counts for a very large number of parameters. As it is the amount of training data that we are usging is quite large to handle on a simple computer. Even if we could get more training data we would not be able to process it in time therefore parameter sharing is important.

Part 2.
We can not model environmental changes, e.g we assume that probability of robot failing to move from row 3 to row 4 is same as movement from row 4 to row 5. Essentially we are assuming that the field is level and that the characteristics of the field like friction, slippage etc. will not vary from row to row.

Part 3.
If we assume that the parameters themeselves have a hyper parameter over them (e.g. the parameters are themselves sampled from the same unimodal beta distribution with a shared beta value). In such a case we could learn potentially differnet values however we would tend to assign them values which are more similar to each other.

Question 4.2
Part 1.

We started by drawing the bayes net and noticed that the Row, Column and Action formed a lattice at the top with the observation being connected only to the row position and column position at that time. We decided to create cliques out of the observations and the row and column positions and 1 clique out of the position row_0, col_0, action_0 and column_1 by addign an edge between row_0 and column_1. We could have created a different clique tree by addign an edge between column_0 and row_1 instead. In that case the sepsets would have contained row_1 instead of col_1 (which is the current sepset)

Part 2 and Part 3.
Running interesection property requires that for any variable, all nodes that contain that variable should be directly connected to each other. We can verify this by looking at the actual clique tree which is of the following form. We can clearly see that none of the variables C_i, R_i, A_i can never occur in disconnected nodes.

(C_0, R_0, A_0, C_1, R_1)---(C_1, R_1, A_1, C_2, R_2)---------(C_t, R_t, A_t)
 +                          +                                  +
 +--(OWN_0, R_0, C_0)       +--(OWN_1, R_1, C_1)               +--(OWN_t, R_t, C_t)
 +--(OWE_0, R_0, C_0)       +--(OWE_1, R_1, C_1)               +--(OWE_t, R_t, C_t)
 .                          .                                  .
 .                          .                                  .
 +--(OL_l_S_0, R_0, C_0)    +--(OL_l_S_1, R_1, C_1)            +--(OL_l_S_t, R_t, C_t)

 
 Part 4.
 Total number of variables = 4l+4+2+1 but they have different perplexity
 If we simply marginalized over joint CPD then the complexity would be = (2^(4l+4)*M*N*4)^T
 To compute distribution over the final prosition at time T using message passing would require
 We can divide the cliques into observation cliques and Positions Cliques.
 For every time t in [1, ..., T] there are 4*L+4 observation cliques.
 The messages from those would be computed by marginalizing out a M*N*2 factor to M*N.
 Every such marginalization would require O(M*N) summations.
 This implies O(M*N*(4*L+4)*T) computations for the messages that are sent from observation cliques.
 And Then the Position Cliques would have to compute their message by multiplying all the messages they received and then marginalizing.
 All Positions cliques receive 4*L+4 message from their observation cliques + 1 message from previous position clique.
 Messages from Observation Cliques have dim [N, M]
 Message from Position Clique have dim [M]
 And the factor in the clique is dim [M*N*4*M*N] so the number of multiplications is M*N*4*M*(L+1)
 This implies that the total order == O(M^2 * N^2LT)
 
 Part 5. We want to query the robot state at time 5 and time 15.
 Part a.
 We will modify the clique tree by creating a single clique out of all the cliques that are between the position cliques at time 5 and 15.
 Since after creating a single clique between state 5 and 15, that clique would contain all 4 of the position variables therefore we would get the correct marginal probability.

 Part b.
 It is not valid this type of query using 2 different clusters because
 then we are making an implicit independence assumption that given
 nothing the two variables are independent. Let's take an
 example. Consider clique tree (A, B)--(B, C)  this clique tree
 encodes the belief that (A \perp C | B) . However if we compute p(A) from
 p(A, B) and p(C) from p(B, C) and multiply them to compute p(A, C)
 then we are saying that [A \perp C] which will be wrong in some cases.
 
 
 Question 4.2.2
 The cliquetree files were generated by running the commands
 python create_cliquetree.py  f/network-grid10x10-t10.txt cliquetree-grid10x10-t10.txt 
 python create_cliquetree.py  f/network-grid10x10-t100.txt cliquetree-grid10x10-t100.txt
 python create_cliquetree.py  f/network-grid10x10-t1000.txt cliquetree-grid10x10-t1000.txt
 python create_cliquetree.py  f/network-grid15x15-t100.txt   cliquetree-grid15x15-t100.txt   
 python create_cliquetree.py  f/network-grid25x25-t100.txt cliquetree-grid25x25-t100.txt
 python create_cliquetree.py  f/network-grid30x30-t100.txt cliquetree-grid30x30-t100.txt
 python create_cliquetree.py  f/network-grid40x40-t100.txt cliquetree-grid40x40-t100.txt

 
Question 4.2.4
We have created a program called bayes-query-sp that answers queries
about marginal distributions of a model using belief propagation.

./bayes-query-sp network_file cpd_file cliquetree_file queries_file

This program requires
class Query(object):
def __init__(self, variable, evidence, is_additive) v is a list, e is a list of tuples, is_additive is bool
get_sorted_clique=tuple(sorted(clique_str.split(",")))
get_required_factors(c) if len(c)==3 then return self.cpd[(c_O(c), c_R(c), c_P(c))] 
                        else assert len(c)==5 then return (self.cpd[c_Rtpp(c), c_Rt(c), c_A(c)], self.cpd[c_Ctpp(c), c_Ct(c), c_A(c)])
Query_Processor
   queries=list_queries(query_filename)
   graph
   process_queries
      for q in queries:
          if q.is_additive self.process_query(q.variable, q.evidence)
          else self.graph.reset() ; self.process_query(q)
   process_query(q)
       return self.graph.process_query(q.variable, q.evidence)
class Clique
     self.belief
     self.is_ready
     self.is_done
     __init__(self, clique_id, initial_belief)
     add_edge(sepset_id) assert one of them is the clique_id itself.
class Sepset __init__(self, endpoints, shared_variables)
     last_message # it must be initialized to one.
class Factor __init__(self, QQQ its a big named table QQQ)  multiply(X, Y) marginalize(variables_to_marginalize)
Graph
   self.clique_set
   self.sepset_set
   self.cpd
   populate_cpd(cpd_file_name)
   
   initialize(cliquetree_file_name)
       create_cliques_and_sepsets(cliquetree_file_name)
       populate_cpd(cpd_file_name)
       self.caliberate()
       self.backup_clique_set=dict(self.clique_set)
       self.backup_sepset_set=dict(self.sepset_set)
   create_cliques_and_sepsets(cliquetree_file_name, cpd_file_name)
       for every clique_str in file
          clique_id=get_sorted_clique(clique_str)
          required_factors=get_required_factors(clique_id)
          initial_belief=reduce(lambda x, y: Factor.multiply(x,y), required_factors)
          self.clique_set[clique_id]=Clique(clique_id, initial_belief)
       for every edge_str in file
          sepset_id=tuple([get_sorted_clique(e) for e in edge_str.split(" -- ")])
          shared_variables=tuple([e for e in sepset_id[0] if e in sepset_id[1]])
          self.sepset_set[sepset_id]=Sepset(sepset_id, shared_variables)
          self.clique_set[sepset_id[0]].add_edge(sepset_id)
          self.clique_set[sepset_id[1]].add_edge(sepset_id)
   process_query(query, evidence=None)
     if evidence is not None:
        self.incrementally_incorporate_evidence(evidence)
        self.caliberate()
     apt_clique_id=choose_appropriate_clique(query)
     v2m=figure_out_variables_to_marginalize(apt_clique_id, query)
     return self.clique_set[apt_clique_id].belief.marginalize(v2m)
   reset() QQQ Must check that this actually works to copy over these objects QQQ
        self.clique_set=copy.deepcopy(self.backup_clique_set)
        self.sepset_set=copy.deepcopy(self.backup_sepset_set)
        
